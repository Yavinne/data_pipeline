ğŸš€ Pipeline ETL - Open Data (Batch Processing)
ğŸ“Œ Objectif du projet

Ce projet illustre la mise en place dâ€™un pipeline ETL complet (Extract â€“ Transform â€“ Load) permettant dâ€™ingÃ©rer, transformer et charger des donnÃ©es ouvertes dans une base de donnÃ©es relationnelle.
Lâ€™objectif est de fournir un exemple concret et rÃ©utilisable dâ€™ingÃ©nierie de donnÃ©es pour des cas dâ€™analyse ultÃ©rieurs (dashboards, APIs, etc.).

ğŸ› ï¸ Stack Technique

Langage : Python 3.10+

Orchestration : Apache Airflow

Stockage : PostgreSQL (peut Ãªtre remplacÃ© par MySQL ou BigQuery)

Transformation : Pandas / SQL

Containerisation : Docker & Docker Compose

Tests : Pytest

ğŸ“‚ Sources de donnÃ©es

Pour ce projet, les donnÃ©es proviennent de lâ€™API Open Data de la qualitÃ© de lâ€™air :
ğŸ‘‰ https://opendata.paris.fr/explore/dataset/

ğŸ“Œ Exemple :

DonnÃ©es brutes : niveaux de pollution journaliers par ville

Format : JSON / CSV

Volume : quelques Mo par jour

ğŸ”„ Architecture du pipeline
flowchart LR
    A[API Open Data] --> B[Airflow Extract Task]
    B --> C[Transform (Pandas)]
    C --> D[Load into PostgreSQL]
    D --> E[Analytical Queries / BI Tools]


Extract : rÃ©cupÃ©ration des donnÃ©es depuis lâ€™API (ou fichier CSV).

Transform : nettoyage (valeurs nulles, typage), enrichissement (calculs de moyennes, agrÃ©gations).

Load : insertion dans une table PostgreSQL normalisÃ©e.

ğŸš€ Lancement du projet
1ï¸âƒ£ Cloner le repo
git clone https://github.com/username/pipeline-etl-opendata.git
cd pipeline-etl-opendata

2ï¸âƒ£ Lancer les services
docker-compose up -d

3ï¸âƒ£ AccÃ©der aux outils

Airflow UI â†’ http://localhost:8080

PostgreSQL â†’ localhost:5432 (user/password dÃ©finis dans .env)

4ï¸âƒ£ Lancer le DAG

Activer le DAG etl_air_quality dans Airflow et exÃ©cuter le pipeline.

ğŸ“Š Exemple de schÃ©ma PostgreSQL
CREATE TABLE air_quality (
    id SERIAL PRIMARY KEY,
    city VARCHAR(255),
    date DATE,
    pm10 FLOAT,
    no2 FLOAT,
    o3 FLOAT,
    created_at TIMESTAMP DEFAULT NOW()
);

âœ… RÃ©sultats attendus

DonnÃ©es brutes extraites quotidiennement

DonnÃ©es transformÃ©es (format homogÃ¨ne, colonnes nettoyÃ©es)

Base PostgreSQL prÃªte pour lâ€™analyse (requÃªtes SQL, BI, dashboards).

ğŸ”® AmÃ©liorations possibles

Ajouter un contrÃ´le qualitÃ© avec Great Expectations

Automatiser les tests unitaires avec CI/CD (GitHub Actions)

DÃ©ployer le pipeline sur GCP/AWS/Azure

CrÃ©er un dashboard Streamlit/Metabase branchÃ© sur PostgreSQL